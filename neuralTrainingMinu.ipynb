{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V28",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyPA4A6Cwn8xX9qMsp/jaJQ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayank22415/chatbot/blob/main/neuralTrainingMinu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import pickle\n",
        "import string\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "UA9ozB8B7jFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GkU9cOm9saTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "rQ9ROUUG_h3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f=open('intents.json','r',errors = 'ignore')\n",
        "#raw=f.read()\n",
        "#raw=raw.lower()\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "#sent_tokens = nltk.sent_tokenize(raw)\n",
        "#word_tokens = nltk.word_tokenize(raw)"
      ],
      "metadata": {
        "id": "CjuinCBpIYRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Oq-dSyUv_jbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents = json.loads(open('intents.json').read())"
      ],
      "metadata": {
        "id": "vow4-9Cr_uiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?','!','.',',']"
      ],
      "metadata": {
        "id": "O9zKOzuHB5cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "   for pattern in intent['patterns']:\n",
        "       word_list = nltk.word_tokenize(pattern)\n",
        "       words.extend(word_list)\n",
        "       documents.append((word_list,intent['tag']))\n",
        "       if intent['tag'] not in classes:\n",
        "           classes.append(intent['tag'])\n"
      ],
      "metadata": {
        "id": "yCMMLqgYCaSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word[0] not in ignore_letters]\n",
        "words = sorted(set(words))"
      ],
      "metadata": {
        "id": "tTQLgpFcLrmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(words)"
      ],
      "metadata": {
        "id": "L3erMVMJ9rXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "metadata": {
        "id": "EoPcdc9aCPke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "output_empty = [0]*len(classes)"
      ],
      "metadata": {
        "id": "rfPh4PYHRV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now i vectorizing the document if word is present it will else it will be zero eg. [how,are,you,doing]\n",
        "document1 - how are you\n",
        "vectorization [1,1,1,0]"
      ],
      "metadata": {
        "id": "DnO4qKbeYnNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "    bag = []\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [ lemmatizer.lemmatize(word.lower())  for word in word_patterns]\n",
        "\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    training.append([bag,output_row])\n"
      ],
      "metadata": {
        "id": "dz31DH1wEeso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "training_x = list(training[:, 0])\n",
        "training_y = list(training[:, 1])\n",
        "\n",
        "training_x = np.array(training_x)"
      ],
      "metadata": {
        "id": "0_Gnzx_UQ72z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128,input_shape=(len(training_x[0]),),activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(training_y[0]),activation='softmax'))"
      ],
      "metadata": {
        "id": "e-TdNZfTVqbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9,nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "r4gaHkA0PPsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(np.array(training_x),np.array(training_y),epochs=300,batch_size=5,verbose=1)\n",
        "model.save('my_model.keras')\n",
        "#model.save('chatbot.h5',hist)\n",
        "print('done')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Zf_3TaNPjMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the model\n",
        "model = load_model('my_model.keras')\n",
        "\n",
        "# Load intents and other data\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "# Load saved words and classes (your vocabulary and labels)\n",
        "import pickle\n",
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Create a bag of words\n",
        "def bow(sentence, words, show_details=True):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(f\"Found in bag: {w}\")\n",
        "    return np.array(bag)\n",
        "\n",
        "# Predict intent\n",
        "def predict_class(sentence):\n",
        "    bow_vector = bow(sentence, words, show_details=False)\n",
        "    res = model.predict(np.array([bow_vector]))[0]\n",
        "    threshold = 0.25\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > threshold]\n",
        "\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "# Get response from intents\n",
        "def get_response(ints, intents_json):\n",
        "    if len(ints) == 0:\n",
        "        return \"I didn't understand that. Can you rephrase?\"\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if i['tag'] == tag:\n",
        "            return random.choice(i['responses'])\n",
        "\n",
        "# Chat function\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg)\n",
        "    res = get_response(ints, intents)\n",
        "    return res\n",
        "\n",
        "# Run chatbot\n",
        "print(\"hii i am minu! Type 'quit' to stop.\")\n",
        "while True:\n",
        "    message = input(\"You: \")\n",
        "    if message.lower() == \"quit\":\n",
        "        break\n",
        "    response = chatbot_response(message)\n",
        "    print(\"Bot:\", response)\n"
      ],
      "metadata": {
        "id": "muK72TBRWydw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}